{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a67a9e4",
   "metadata": {},
   "source": [
    "# ETL Transformation Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146bc4dd",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7efbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f07733",
   "metadata": {},
   "source": [
    "## Load extracted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e46752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 raw records and 10 incremental records\n"
     ]
    }
   ],
   "source": [
    "rawData = pd.read_csv('data/raw_data.csv')\n",
    "incrementalData = pd.read_csv('data/incremental_data.csv')\n",
    "print(f\"Loaded {len(rawData)} raw records and {len(incrementalData)} incremental records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac021330",
   "metadata": {},
   "source": [
    "## Transfromation 1: Cleaning missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2345aae8",
   "metadata": {},
   "source": [
    "#### Creating compies for Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4d4e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawCleaned = rawData.copy()\n",
    "incCleaned = incrementalData.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b30665",
   "metadata": {},
   "source": [
    "#### Before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4253d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - Missing values: 88\n",
      "Raw data - Duplicates: 1\n",
      "Incremental data - Missing values: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw data - Missing values: {rawData.isnull().sum().sum()}\")\n",
    "print(f\"Raw data - Duplicates: {rawData.duplicated().sum()}\")\n",
    "print(f\"Incremental data - Missing values: {incrementalData.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c33c1be",
   "metadata": {},
   "source": [
    "#### Missing cutomer names\n",
    "- change the missing names to Unknown for both raw and incremental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eca79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawCleaned['customer_name'].fillna('Unknown_Customer', inplace=True)\n",
    "incCleaned['customer_name'].fillna('Unknown_Customer', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4019a74c",
   "metadata": {},
   "source": [
    "#### Imputing missing quantities for raw data using median\n",
    "- this is beacause median values are less affected by outliers\n",
    "- calculates the median of the quantity column and fills missing values with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e1db9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in rawCleaned['product'].unique():\n",
    "    product_median_qty = rawCleaned[rawCleaned['product'] == product]['quantity'].median()\n",
    "    rawCleaned.loc[(rawCleaned['product'] == product) & (rawCleaned['quantity'].isna()), 'quantity'] = product_median_qty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1f9f4",
   "metadata": {},
   "source": [
    "#### Imputing missing values for incremental data using median \n",
    "- used the same logic as above\n",
    "- calculates the median of the quantity column and fills missing values with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db6f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for product in incCleaned['product'].unique():\n",
    "    product_median_qty = incCleaned[incCleaned['product'] == product]['quantity'].median()\n",
    "    incCleaned.loc[(incCleaned['product'] == product) & (incCleaned['quantity'].isna()), 'quantity'] = product_median_qty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1021a75f",
   "metadata": {},
   "source": [
    "#### missing unit prices using median\n",
    "- first created a price mapping dictionary to hold the median prices for each product\n",
    "- used the apply() logic to replace missing unit prices with the median price for that product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e75fc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_map = rawCleaned.groupby('product')['unit_price'].median().to_dict()\n",
    "rawCleaned['unit_price'] = rawCleaned.apply(\n",
    "    lambda row: price_map[row['product']] if pd.isna(row['unit_price']) else row['unit_price'], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad755bf",
   "metadata": {},
   "source": [
    "#### Missing regions\n",
    "- created most common region mapping dictionary using the mode() function\n",
    "- used the fillna() function to replace missing regions with the most common region for that product\n",
    "- applied this to both raw and incremental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee274c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping\n",
    "region_map = rawCleaned.dropna().groupby('customer_name')['region'].agg(lambda x: x.mode()[0] if not x.mode().empty else 'Unknown').to_dict()\n",
    "# Fill missing regions\n",
    "rawCleaned['region'] = rawCleaned['region'].fillna(rawCleaned['customer_name'].map(region_map)).fillna('Unknown')\n",
    "incCleaned['region'] = incCleaned['region'].fillna('Central')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11130df0",
   "metadata": {},
   "source": [
    "#### Removing Exact Duplicates\n",
    "- using the drop_duplicates() function to remove exact duplicates from both raw and incremental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe46992",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawCleaned.drop_duplicates(inplace=True)\n",
    "incCleaned.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5965a7",
   "metadata": {},
   "source": [
    "#### Cleaning verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c6de77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data - Missing values: 1\n",
      "Raw data - Duplicates: 0\n",
      "Raw data - Records: 100 → 99\n"
     ]
    }
   ],
   "source": [
    "print(f\"Raw data - Missing values: {rawCleaned.isnull().sum().sum()}\")\n",
    "print(f\"Raw data - Duplicates: {rawCleaned.duplicated().sum()}\")\n",
    "print(f\"Raw data - Records: {len(rawData)} → {len(rawCleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4ae55",
   "metadata": {},
   "source": [
    "## Transfromation 2: Data Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d35bb2",
   "metadata": {},
   "source": [
    "#### Feature Engineering: Adding total price columns to botb raw and incremental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48503016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa9b702f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
